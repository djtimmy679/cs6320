{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf5c9d718bd760f",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5818b75b131917cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T20:08:52.157469Z",
     "start_time": "2024-04-29T20:08:37.075834Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tqdm vertexai sentencepiece matplotlib\n",
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7bb53",
   "metadata": {},
   "source": [
    "## Put Data in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# # Directory containing the data files\n",
    "# data_dir = \"./data/middle\"\n",
    "\n",
    "# # Initialize an empty list to store data\n",
    "# all_data = []\n",
    "\n",
    "# # Loop through all files in the data directory\n",
    "# for filename in os.listdir(data_dir):\n",
    "#     if filename.endswith(\".txt\"):\n",
    "#         # Read the JSON content of the file\n",
    "#         with open(os.path.join(data_dir, filename), \"r\") as file:\n",
    "#             file_data = json.load(file)\n",
    "        \n",
    "#         # Append the data to the list\n",
    "#         all_data.append(file_data)\n",
    "\n",
    "# # Write the data to a JSON file\n",
    "# with open(\"middle.json\", \"w\") as json_file:\n",
    "#     json.dump(all_data, json_file, indent=4)\n",
    "import json\n",
    "\n",
    "# Parameters\n",
    "input_file = \"train.jsonl\"  # Input file with data\n",
    "\n",
    "# Load the JSONL data from the file and calculate passage lengths\n",
    "total_length = 0\n",
    "num_entries = 0\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        entry = json.loads(line)\n",
    "        passage_length = len(entry['passage'])\n",
    "        total_length += passage_length\n",
    "        num_entries += 1\n",
    "\n",
    "# Compute the average passage length\n",
    "if num_entries > 0:\n",
    "    average_length = total_length / num_entries\n",
    "    print(f\"Average Passage Length: {average_length}\")\n",
    "else:\n",
    "    print(\"No entries found in the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb2f0dde754152",
   "metadata": {},
   "source": [
    "## Summarize paragraph -> Answer Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb5bb2a2c2d56a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T21:17:40.837640Z",
     "start_time": "2024-04-28T17:53:27.544028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 928/9426 [25:25<2:48:39,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error summarizing passage for question is todd bethany's dad in coronation street: Response has no candidates (and no text).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 1800/9426 [47:48<3:05:34,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error answering question is 16 the age of consent in canada: Response has no candidates (and no text).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  28%|██▊       | 2680/9426 [1:10:29<2:46:17,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error summarizing passage for question is straight talk the same as total wireless: Content has no parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▋      | 3438/9426 [1:30:08<2:32:27,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error summarizing passage for question is there a white castle restaurant in california: Content has no parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 3552/9426 [1:33:06<2:07:23,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error summarizing passage for question does anna have a baby in 50 shades freed: Response has no candidates (and no text).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|████▎     | 4017/9426 [1:44:58<2:34:59,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error summarizing passage for question does anna get pregnant in fifty shades freed: Response has no candidates (and no text).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|█████▉    | 5616/9426 [2:26:16<1:27:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error summarizing passage for question has anyone ever won the moment of truth: Response has no candidates (and no text).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 5939/9426 [2:34:45<1:24:51,  1.46s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from vertexai import generative_models\n",
    "\n",
    "# Parameters\n",
    "input_file = \"train.jsonl\"  # Input file with data\n",
    "output_file = \"results_train.jsonl\"  # Output file\n",
    "summary_query = \"Summarize this paragraph to make it as concise as possible, capturing the main ideas clearly and briefly without losing information. Provide the edited paragraph as-is without any additional context, labels, headings, or formatting.\"\n",
    "# Load the JSONL data from the file\n",
    "data = []\n",
    "with open(input_file, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding here\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Model setup for summarization\n",
    "summary_model = GenerativeModel(\"gemini-1.0-pro-002\")\n",
    "answer_model = GenerativeModel(\"gemini-1.0-pro-002\")  # Assuming the same model for summarization and answering\n",
    "safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "limit = -1\n",
    "# Process each item\n",
    "output_data = []\n",
    "for item in tqdm(data[:limit], desc=\"Processing\"):\n",
    "    # Generate concise summary\n",
    "    try:\n",
    "        summary_response = summary_model.generate_content(\n",
    "            f\"{summary_query} {item['passage']}\",\n",
    "            generation_config={\"max_output_tokens\": 150, \"temperature\": 0.5},\n",
    "            safety_settings=safety_config)\n",
    "        item['passage_summarized'] = summary_response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing passage for question {item['question']}: {str(e)}\")\n",
    "        item['passage_summarized'] = \"Error summarizing\"\n",
    "        continue\n",
    "\n",
    "    # Answer question based on summary\n",
    "    answer_query = f\"{item['passage_summarized']}. Based on this summary, answer the following question with a simple 'true' or 'false'. '{item['question']}'. Provide the response as-is without any additional context, labels, headings, or formatting:\"\n",
    "    try:\n",
    "        answer_response = answer_model.generate_content(\n",
    "            answer_query,\n",
    "            generation_config={\"max_output_tokens\": 5, \"temperature\": 0},\n",
    "            safety_settings=safety_config)\n",
    "        item['model_answer'] = answer_response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error answering question {item['question']}: {str(e)}\")\n",
    "        item['model_answer'] = \"Error answering\"\n",
    "\n",
    "    output_data.append(item)\n",
    "\n",
    "# Write the updated data to a JSONL file\n",
    "with open(output_file, 'w') as file:\n",
    "    for line in output_data:\n",
    "        json.dump(line, file)\n",
    "        file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db076b9bc89a968",
   "metadata": {},
   "source": [
    "## Visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f380b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 30\n",
      "Correct answers: 26\n",
      "Accuracy: 86.67%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Load the JSON data from the processed file\n",
    "input_file = 'results_train.jsonl'  # Adjust the path as needed\n",
    "data = []\n",
    "with open(input_file, 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding here\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "total_questions = 0\n",
    "correct_answers = 0\n",
    "\n",
    "# Iterate through each entry in the JSON data\n",
    "for item in data:\n",
    "    # Retrieve the list of correct answers and model-generated answers\n",
    "    correct = item['answer']\n",
    "    model_generated = item['model_answer']\n",
    "\n",
    "    # Increment the total question count\n",
    "    total_questions += 1\n",
    "\n",
    "    # Compare each answer with the model answer and count correct ones\n",
    "    model_answer_bool = model_generated.lower() == 'true'  # Convert string to Boolean\n",
    "    if correct == model_answer_bool:\n",
    "        correct_answers += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = (correct_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "# Output the results\n",
    "print(f'Total questions: {total_questions}')\n",
    "print(f'Correct answers: {correct_answers}')\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
